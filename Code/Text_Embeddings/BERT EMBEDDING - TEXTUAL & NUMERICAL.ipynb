{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dd450ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ca5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'RATING',\n",
       "       'VERIFIED_PURCHASE', 'REVIEW_TITLE', 'REVIEW_TEXT', 'NUM_NOUNS',\n",
       "       'NUM_VERBS', 'NUM_ADJECTIVES', 'NUM_ADVERBS', 'REVIEW_LENGTH',\n",
       "       'SENTIMENT_SCORE', 'TITLE_LENGTH', 'AVERAGE_RATING', 'RATING_DEVIATION',\n",
       "       'NUM_REVIEWS', 'READABILITY_FRE', 'SENTIMENT_CATEGORY_ENCODED',\n",
       "       'RATING_CATEGORY_ENCODED', 'COHERENT_ENCODED', 'AVG_WORD_LENGTH',\n",
       "       'LABEL_ENCODED', 'NUM_NAMED_ENTITIES', 'CAPITAL_CHAR_COUNT',\n",
       "       'PUNCTUATION_COUNT', 'PREPROCESSED_REVIEW_TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"amazon_reviews_3.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1407a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = df['REVIEW_TEXT']\n",
    "numerical_features = df[[\n",
    "        'RATING',\n",
    "       'VERIFIED_PURCHASE', 'NUM_NOUNS',\n",
    "       'NUM_VERBS', 'NUM_ADJECTIVES', 'NUM_ADVERBS', 'REVIEW_LENGTH',\n",
    "       'SENTIMENT_SCORE', 'TITLE_LENGTH', 'AVERAGE_RATING', 'RATING_DEVIATION',\n",
    "       'NUM_REVIEWS', 'READABILITY_FRE', 'SENTIMENT_CATEGORY_ENCODED',\n",
    "       'RATING_CATEGORY_ENCODED', 'COHERENT_ENCODED', 'AVG_WORD_LENGTH',\n",
    "       'LABEL_ENCODED', 'NUM_NAMED_ENTITIES', 'CAPITAL_CHAR_COUNT',\n",
    "       'PUNCTUATION_COUNT', 'PREPROCESSED_REVIEW_TEXT'\n",
    "]]\n",
    "labels = df['LABEL_ENCODED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407ad5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD BERT TOKENIZER\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb58f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZE AND ENCODE THE TEXTUAL FEATURES\n",
    "text_tokens = tokenizer.batch_encode_plus(\n",
    "    text_features,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41fe508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TOKENISED INPUTSINTO TENSORFLOW TENSORS\n",
    "input_ids = text_tokens['input_ids']\n",
    "attention_mask = text_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db215172",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_numerical_features = len(numeric_features)\n",
    "max_sequence_length = 0\n",
    "for text in text_features:\n",
    "    if len(text.split()) > max_sequence_length:\n",
    "        max_sequence_length = len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69516b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP LEARNING MODEL ARCHITECTURE\n",
    "\n",
    "# INPUT LAYERS FOR NUMERICAL FEATURES AND BERT INPUTS\n",
    "numerical_input = Input(shape=(num_numerical_features,))\n",
    "bert_input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32)\n",
    "bert_attention_mask = Input(shape=(max_sequence_length,), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27bd1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE BERT MODEL\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c068710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE THE BERT EMBEDDINGS\n",
    "bert_embeddings = bert_model(bert_input_ids, attention_mask=bert_attention_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c56a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLATTEN THE BERT EMBEDDINGS\n",
    "flattened_bert = tf.keras.layers.Flatten()(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55995fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCATENATE THE NUMERICAL FEATURES AND FLATTENED BERT EMBEDDING\n",
    "concatenated_features = Concatenate()([numerical_input, flattened_bert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4676a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDITIONAL LAYERS\n",
    "dense_layer = Dense(128, activation='relu')(concatenated_features)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66877f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE MODEL\n",
    "model = Model(inputs=[numerical_input, bert_input_ids, bert_attention_mask], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c71c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILE THE MODEL\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4814d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "#FLOATS ARE COMPATIBLE WITH TENSORFLOW; NOT INTEGERS\n",
    "labels = np.array(labels, dtype=np.float32)\n",
    "numerical_features = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dae62341",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(input_ids, maxlen=max_sequence_length, padding='post')\n",
    "attention_mask = pad_sequences(attention_mask, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afb7c17c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/backend.py\", line 3581, in concatenate\n        return tf.concat([to_dense(x) for x in tensors], axis)\n\n    ValueError: Exception encountered when calling layer 'concatenate' (type Concatenate).\n    \n    Shape must be rank 1 but is rank 2 for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](IteratorGetNext, model/flatten/Reshape, model/concatenate/concat/axis)' with input shapes: [16], [16,1038336], [].\n    \n    Call arguments received by layer 'concatenate' (type Concatenate):\n      â€¢ inputs=['tf.Tensor(shape=(16,), dtype=float32)', 'tf.Tensor(shape=(16, 1038336), dtype=float32)']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TRAIN THE MODEL WITH: NUMERICAL FEATURES, BERT INPUT AND LABELS\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/__autograph_generated_fileh3diie__.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/kpandey/anaconda3/lib/python3.10/site-packages/keras/backend.py\", line 3581, in concatenate\n        return tf.concat([to_dense(x) for x in tensors], axis)\n\n    ValueError: Exception encountered when calling layer 'concatenate' (type Concatenate).\n    \n    Shape must be rank 1 but is rank 2 for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](IteratorGetNext, model/flatten/Reshape, model/concatenate/concat/axis)' with input shapes: [16], [16,1038336], [].\n    \n    Call arguments received by layer 'concatenate' (type Concatenate):\n      â€¢ inputs=['tf.Tensor(shape=(16,), dtype=float32)', 'tf.Tensor(shape=(16, 1038336), dtype=float32)']\n"
     ]
    }
   ],
   "source": [
    "#TRAIN THE MODEL WITH: NUMERICAL FEATURES, BERT INPUT AND LABELS\n",
    "model.fit([numerical_features, input_ids, attention_mask], labels, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b27111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
