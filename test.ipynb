{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da01975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, FunctionTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc1649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#CREATE DATAFRAME FROM THE FILE\n",
    "df = pd.read_csv('amazon_reviews.txt', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21db00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT THE LABELS TO FAKE & NON FAKE\n",
    "df.loc[df[\"LABEL\"] == \"__label1__\", \"LABEL\"] = 'Fake'\n",
    "df.loc[df[\"LABEL\"] == \"__label2__\", \"LABEL\"] = 'Not Fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count nouns in a text\n",
    "def count_nouns(Pos_counts):\n",
    "    noun_count = Pos_counts.get(spacy.parts_of_speech.NOUN, 0)\n",
    "    return noun_count\n",
    "\n",
    "# Function to count verbs in a text\n",
    "def count_verbs(Pos_counts):\n",
    "    verb_count = Pos_counts.get(spacy.parts_of_speech.VERB, 0)\n",
    "    return verb_count\n",
    "\n",
    "# Function to count adjectives in a text\n",
    "def count_adjectives(Pos_counts):\n",
    "    adjective_count = Pos_counts.get(spacy.parts_of_speech.ADJ, 0)\n",
    "    return adjective_count\n",
    "\n",
    "# Function to count adverbs in a text\n",
    "def count_adverbs(Pos_counts):\n",
    "    adverb_count = Pos_counts.get(spacy.parts_of_speech.ADV, 0)\n",
    "    return adverb_count\n",
    "\n",
    "def pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    Pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    return Pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5fb02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_5595/3662947513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_COUNTS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'REVIEW_TEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM NOUNS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_COUNTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_nouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM VERBS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_COUNTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_verbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM ADJECTIVES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_COUNTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_adjectives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM ADVERBS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_COUNTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_adverbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_5595/1309595354.py\u001b[0m in \u001b[0;36mpos_counts\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mPos_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPos_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/thinc/describe.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/thinc/neural/mem.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['POS_COUNTS'] = df['REVIEW_TEXT'].apply(pos_counts)\n",
    "df['NUM NOUNS'] = df['POS_COUNTS'].apply(count_nouns)\n",
    "df['NUM VERBS'] = df['POS_COUNTS'].apply(count_verbs)\n",
    "df['NUM ADJECTIVES'] = df['POS_COUNTS'].apply(count_adjectives)\n",
    "df['NUM ADVERBS'] = df['POS_COUNTS'].apply(count_adverbs)\n",
    "df['REVIEW_LENGTH'] = df['REVIEW_TEXT'].apply(lambda d: len(d))\n",
    "df['SENTIMENT SCORE'] = df['REVIEW_TEXT'].apply(lambda d: sid.polarity_scores(d)['compound'])\n",
    "label_encoder = LabelEncoder()\n",
    "df['VERIFIED_PURCHASE'] = label_encoder.fit_transform(df['VERIFIED_PURCHASE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fde97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('amazon_reviews.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cosine similarity between two texts\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    # Tokenize and convert the texts to spaCy Doc objects\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "\n",
    "    # Get the word vectors for each token in the documents\n",
    "    vec1 = doc1.vector.reshape(1, -1)\n",
    "    vec2 = doc2.vector.reshape(1, -1)\n",
    "\n",
    "    # Calculate the cosine similarity between the vectors\n",
    "    similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463588e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_reviews.tsv', sep = '\\t')\n",
    "# Get random 1000 records\n",
    "df1 = df.sample(n=1000, random_state=42)  # Make a copy of the top 1000 records as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5051923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eDOC_ID', 'LABEL', 'RATING', 'VERIFIED_PURCHASE', 'PRODUCT_CATEGORY',\n",
       "       'PRODUCT_ID', 'PRODUCT_TITLE', 'REVIEW_TITLE', 'REVIEW_TEXT',\n",
       "       'POS_COUNTS', 'NUM NOUNS', 'NUM VERBS', 'NUM ADJECTIVES', 'NUM ADVERBS',\n",
       "       'REVIEW_LENGTH', 'SENTIMENT SCORE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea00d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ADD A MAXIMUM SIMILARITY COLUMN\n",
    "tfidfvectoriser = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44300a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bc811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9e23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "293ba4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7466d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[['RATING', 'VERIFIED_PURCHASE', 'REVIEW_TEXT', 'NUM NOUNS', 'NUM VERBS','NUM ADJECTIVES', 'NUM ADVERBS', 'REVIEW_LENGTH', 'SENTIMENT SCORE']]\n",
    "Y = df1['LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d93944f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(\n",
    "        lambda x: x['REVIEW_TEXT'], validate=False)),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "])\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(\n",
    "        lambda x: x[['NUM NOUNS','NUM VERBS','NUM ADVERBS','NUM ADJECTIVES','VERIFIED_PURCHASE', 'RATING', 'REVIEW_LENGTH', 'SENTIMENT SCORE']], validate=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('text_features', text_pipeline),\n",
    "    ('numeric_features', numeric_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f0d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1491f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04cc4216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py:209: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(joblib_version) < '0.12':\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py:209: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(joblib_version) < '0.12':\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #GET THE SPLIT TRAINING AND TESTING DATA: 80-20\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X,Y, test_size=0.2, random_state=42) #RANDOMLY SELECT BECAUSE THEY ARE CONTINOUSLY ARRANGED\n",
    "\n",
    "\n",
    "# # Fit and transform the training data\n",
    "X_TRAIN_TRANSFORMED = feature_union.fit_transform(X_TRAIN, Y_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c45c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       eDOC_ID     LABEL  RATING  VERIFIED_PURCHASE        PRODUCT_CATEGORY  \\\n",
      "2312      2313      Fake       3                  0                   Shoes   \n",
      "16845    16846  Not Fake       4                  1             Electronics   \n",
      "6105      6106      Fake       5                  0                      PC   \n",
      "18082    18083  Not Fake       1                  1                    Toys   \n",
      "11335    11336  Not Fake       2                  1               Furniture   \n",
      "17677    17678  Not Fake       5                  1                 Kitchen   \n",
      "1092      1093      Fake       4                  0         Lawn and Garden   \n",
      "8364      8365      Fake       5                  0               Video DVD   \n",
      "18550    18551  Not Fake       1                  1     Musical Instruments   \n",
      "11423    11424  Not Fake       5                  1               Video DVD   \n",
      "6757      6758      Fake       5                  0                    Toys   \n",
      "12893    12894  Not Fake       5                  1                  Camera   \n",
      "17446    17447  Not Fake       5                  1      Home Entertainment   \n",
      "11823    11824  Not Fake       5                  1                   Shoes   \n",
      "4772      4773      Fake       5                  0        Home Improvement   \n",
      "18882    18883  Not Fake       5                  1     Musical Instruments   \n",
      "7700      7701      Fake       5                  0                 Apparel   \n",
      "5018      5019      Fake       5                  0                Wireless   \n",
      "1474      1475      Fake       4                  0                   Tools   \n",
      "4966      4967      Fake       5                  1                 Kitchen   \n",
      "6241      6242      Fake       5                  0                    Baby   \n",
      "474        475      Fake       3                  1            Pet Products   \n",
      "19606    19607  Not Fake       5                  1                 Watches   \n",
      "3316      3317      Fake       4                  0              Automotive   \n",
      "2714      2715      Fake       4                  0                 Apparel   \n",
      "8918      8919      Fake       5                  0     Musical Instruments   \n",
      "5701      5702      Fake       5                  0                   Tools   \n",
      "5948      5949      Fake       5                  0              Automotive   \n",
      "4059      4060      Fake       2                  1         Lawn and Garden   \n",
      "19103    19104  Not Fake       4                  1                 Watches   \n",
      "...        ...       ...     ...                ...                     ...   \n",
      "6595      6596      Fake       5                  0                    Toys   \n",
      "4309      4310      Fake       5                  1                Wireless   \n",
      "3958      3959      Fake       3                  0     Musical Instruments   \n",
      "19404    19405  Not Fake       1                  1  Health & Personal Care   \n",
      "16400    16401  Not Fake       4                  1             Electronics   \n",
      "1272      1273      Fake       3                  0                Wireless   \n",
      "3189      3190      Fake       3                  0               Video DVD   \n",
      "13293    13294  Not Fake       5                  1            Pet Products   \n",
      "20497    20498  Not Fake       1                  1      Home Entertainment   \n",
      "8887      8888      Fake       5                  0                 Luggage   \n",
      "1475      1476      Fake       3                  0                   Shoes   \n",
      "2533      2534      Fake       2                  0                    Baby   \n",
      "13674    13675  Not Fake       4                  1             Electronics   \n",
      "12004    12005  Not Fake       4                  0               Furniture   \n",
      "14480    14481  Not Fake       5                  1                 Grocery   \n",
      "3836      3837      Fake       2                  0                    Baby   \n",
      "13188    13189  Not Fake       5                  1        Home Improvement   \n",
      "3472      3473      Fake       4                  0                   Shoes   \n",
      "5735      5736      Fake       5                  0                   Books   \n",
      "16532    16533  Not Fake       5                  1                 Grocery   \n",
      "20188    20189  Not Fake       1                  1      Home Entertainment   \n",
      "606        607      Fake       1                  1        Home Improvement   \n",
      "17601    17602  Not Fake       3                  0                Outdoors   \n",
      "2826      2827      Fake       1                  1        Home Improvement   \n",
      "13925    13926  Not Fake       5                  1               Furniture   \n",
      "1338      1339      Fake       4                  0                   Books   \n",
      "484        485      Fake       4                  0                    Baby   \n",
      "1719      1720      Fake       3                  0        Home Improvement   \n",
      "19712    19713  Not Fake       4                  1      Home Entertainment   \n",
      "218        219      Fake       4                  0               Furniture   \n",
      "\n",
      "       PRODUCT_ID                                      PRODUCT_TITLE  \\\n",
      "2312   B001NIZ7OC  Sunbelt Men's Neptune 190 Polarized Sunglasses...   \n",
      "16845  B000RSOV50  La Crosse Technology BC-700 Alpha Power Batter...   \n",
      "6105   B00MJKE0AU  Wacom Intuos Pen Small Tablet (CTL480) (Certif...   \n",
      "18082  B0007YDBKK                        Disney Mickey Pencil Holder   \n",
      "11335  B00DPJ4W1Q                   Big Joe Cuddle Chair, Spicy Lime   \n",
      "17677  B003ZHU8M0  Cuisipro Surface Glide Technology 4-Sided Boxe...   \n",
      "1092   B001UQ6WKU  Makita BUB182Z 18-Volt LXT Lithium-Ion Cordles...   \n",
      "8364   B008645XS6                                           Silenced   \n",
      "18550  B00E1P3C5U                    Jack Mount - Aluminum, For Tele   \n",
      "11423  B00007GZS0    A.M. & P.M. Yoga - Conditioning For Weight Loss   \n",
      "6757   B006HDQLQQ  Double Horse 3.5CH Large Size Outdoor 9053G Gy...   \n",
      "12893  B0055E8HZG   Micover Slipover Windscreen for RODE VideoMic VM   \n",
      "17446  B00BUCLVZU             TiVo Mini with IR Remote (Old Version)   \n",
      "11823  B001HN6S7I  Madden Girl Women's Sanguine Boot,Brown Paris,...   \n",
      "4772   B00BNGY7E0  Vault Locks 3200 Key Storage Lock Box with Set...   \n",
      "18882  B005HGM1D6  Hosa CMP-159 3.5 mm TRS to Dual 1/4 inch TS St...   \n",
      "7700   B00FDKN4PA  Villydan HOT Sexy Lingerie Stylish Fishnet Bod...   \n",
      "5018   B00E8GYISW  Apple iPhone 5 5S Slim Case: Amplim High Defin...   \n",
      "1474   B007C6LHXY  Craftsman 9-31794 Slotted Phillips Screwdriver...   \n",
      "4966   B00KNK0M1O                Savvy Infusion Water Bottle - 24 Oz   \n",
      "6241   B00643FPKW  Philips AVENT Nighttime Disposable Breast Pads...   \n",
      "474    B00K5O6YXI  #1 Rated Dog Training Collar with Remote Clear...   \n",
      "19606  B003P1OCW8  Bulova Men's 96B127 Precisionist Claremont Bla...   \n",
      "3316   B000NVW1LM     ATP AT-205 Re-Seal Stops Leaks, 8 Ounce Bottle   \n",
      "2714   B001HO5J90             Air Mail Par Avion Tyvek Mighty Wallet   \n",
      "8918   B0071I48GG  Pyle PWMA430U - Wireless Rechargeable Portable...   \n",
      "5701   B001EHEFT0   Snap-On 870111 16-Inch Utility Tool Tote Carrier   \n",
      "5948   B004ETGQKG                            JH Smith Car Snow Cover   \n",
      "4059   B004P3LDEU  INTEX Cleaning Maintenance Swimming Pool Kit w...   \n",
      "19103  B00322PK3K  Swatch Women's GO105 Quartz Orange Dial Plasti...   \n",
      "...           ...                                                ...   \n",
      "6595   B00M159GCK  Ginzick 4ch Rc Remote Control Speed Zoom Race ...   \n",
      "4309   B00MBUUZZM  Note 3 Armband: Stalion® Sports Running & Exer...   \n",
      "3958   B004QO2TWI           First Act FG1106 Natural Acoustic Guitar   \n",
      "19404  B002A172JE            Kava Kava Muscle Relaxant and Sleep Aid   \n",
      "16400  B0080R95XI  Ceptics Grounded Universal Plug Adapter for UK...   \n",
      "1272   B008ALV0V4  Aduro UNI-SPH01-CM U-Grip Plus Universal Dashb...   \n",
      "3189   B003ZSJ212  Star Wars: The Complete Saga (Episodes I-VI) (...   \n",
      "13293  B0010P0YSW  Nylabone Dura Chew Regular Original Flavored W...   \n",
      "20497  B004N85YD8  Samsung BD-D6700 3D Blu-ray Disc Player (Silve...   \n",
      "8887   B0072073XI  large portemonnaie wallet STARS - purse - genu...   \n",
      "1475   B001GLGW0O  FASH Giraffe Print Faux Leather Tote Shoulder ...   \n",
      "2533   B001OC5UNK  Regalo Easy Open 50 Inch Wide Baby Gate, Press...   \n",
      "13674  B00133NQ9A  Winegard HDA-100 Distribution Amplifier 5-1000...   \n",
      "12004  B003G2ZKZO  Best Selling Brighton Cloth Storage Ottoman, O...   \n",
      "14480  B007OSBHNA  Brown Gold, Coffee Variety Pack, 36 Single Ser...   \n",
      "3836   B000H1MRJO             Regalo My Cot Portable Bed, Royal Blue   \n",
      "13188  B003BIGDKE  GE WR60X190 Refrigerator Evaporator Fan Motor ...   \n",
      "3472   B007N99QHY  MG Collection Samantha Weave Belt Hobo Handbag...   \n",
      "5735   1460968999  Atavistic Metamorphosis: A NEW AND LOGICAL EXP...   \n",
      "16532  B009GFWCAG  Heinz Home Style Sloppy Joe Sauce, Classic, 15...   \n",
      "20188  B00HPMCPJK  Sony BDPS1200 Blu-Ray Disc Player, Wired (2014...   \n",
      "606    B00G7KHEOC  KINGLAKE® Battery Powered LCD CO Carbon Monoxi...   \n",
      "17601  B001J72EXE    Helly Hansen Women's W 3/4 Pant (Black, Medium)   \n",
      "2826   B00MSGYOOM  rozin Deck Mount One Single Handle Square Bath...   \n",
      "13925  B00QMNWGSS  Stony-Edge No Assembly Folding Bookcase, 4 She...   \n",
      "1338   1482623234  Matcha Green Tea Superfood: How A Miraculous T...   \n",
      "484    B004AHMCLY               Sassy Developmental Sensory Ball Set   \n",
      "1719   B0018P1U1Y  Hardware House 543728 Berkshire 21-Inch by 18-...   \n",
      "19712  B003VQQVCG  VIZIO E320VP 32-Inch LED LCD HDTV, Black (2010...   \n",
      "218    B000NPOOI6             Winsome Wood Genoa End Table, Espresso   \n",
      "\n",
      "                                            REVIEW_TITLE  \\\n",
      "2312                              Happy with my purchase   \n",
      "16845              Amazing Charger. Almost everything!!!   \n",
      "6105                                             5 star!   \n",
      "18082                                               Junk   \n",
      "11335                            Not enough tushie space   \n",
      "17677                              Ultimate hand grater!   \n",
      "1092                                   Excellent product   \n",
      "8364                                              Moving   \n",
      "18550                              Disappointing Product   \n",
      "11423                                        Perfect....   \n",
      "6757                                   Huge! We love it!   \n",
      "12893                                           AMAZING.   \n",
      "17446                I have been pretty happy with these   \n",
      "11823                                         Super Cute   \n",
      "4772                                         Very secure   \n",
      "18882                              Exactly as advertised   \n",
      "7700   The material feels great against my skin and m...   \n",
      "5018                                             Perfect   \n",
      "1474                      A great screwdriver to own....   \n",
      "4966                     This water bottle is so great!!   \n",
      "6241                                        Very helpful   \n",
      "474    Only product that stopped our dog from chewing...   \n",
      "19606          Been burnt before but this watch is great   \n",
      "3316                                               Works   \n",
      "2714                                    Worked perfectly   \n",
      "8918                                          2nd choice   \n",
      "5701                                         easy access   \n",
      "5948                                            It works   \n",
      "4059                                Good for intex pools   \n",
      "19103  The band discolored pretty quickly, but got re...   \n",
      "...                                                  ...   \n",
      "6595                                      Wow! It's cool   \n",
      "4309                  Finding it easy to train with this   \n",
      "3958   First Act Discovery Natural Acoustic Guitar - ...   \n",
      "19404                                        Not so much   \n",
      "16400             Functional and cost-effective adapters   \n",
      "1272                                              Failed   \n",
      "3189                                             Oh well   \n",
      "13293                              Yes- these are great!   \n",
      "20497                                                POS   \n",
      "8887                                     i love the time   \n",
      "1475                                       short handles   \n",
      "2533                           so cheap and poor quality   \n",
      "13674                                         Works well   \n",
      "12004  A rather nice ottoman but needs to be taller t...   \n",
      "14480                                             coffie   \n",
      "3836                            it is not comfortable...   \n",
      "13188                       Fixed my fridge on the cheap   \n",
      "3472                                             Love it   \n",
      "5735   This book is going to help in finding a cure f...   \n",
      "16532                                         Five Stars   \n",
      "20188                                       Sony is Sh!*   \n",
      "606                                        Does not work   \n",
      "17601                    Legs great, waistband too loose   \n",
      "2826   Not good. faucets I bought have a leak and can...   \n",
      "13925                                     Great Bookcase   \n",
      "1338                          is this good for diabetics   \n",
      "484                                                 ball   \n",
      "1719                                     Looks expensive   \n",
      "19712                                    Picture perfect   \n",
      "218                           Decent value for the money   \n",
      "\n",
      "                                             REVIEW_TEXT  \\\n",
      "2312   Just received my pair of sunglasses in the mai...   \n",
      "16845  This charger brought back to life over two doz...   \n",
      "6105   I bought this to doodle around a little bit on...   \n",
      "18082  Item came broken.  Mickey was off the base.  I...   \n",
      "11335  I researched for weeks for a cuddle spot for m...   \n",
      "17677  This is simply the most fabulous hand grater I...   \n",
      "1092   Very useful product for cleaning PC, windows, ...   \n",
      "8364   I have watched this movie when I was in Korea....   \n",
      "18550  This is a very cheap product.  Threading was t...   \n",
      "11423  The morning routine is just the right amount o...   \n",
      "6757   very fun for the whole entire family. Can fly ...   \n",
      "12893  Great. The best one in the market. Looks amazi...   \n",
      "17446  I have been pretty happy with these. I have tw...   \n",
      "11823  My first paitr of cow boy boots and I Love the...   \n",
      "4772   Shocked at how heavy duty and secure this lock...   \n",
      "18882  The cable was as advertised and very cost-effe...   \n",
      "7700   This fit perfectly. I'm quite slim so I didn't...   \n",
      "5018   I wanted a very basic case for my new iPhone 5...   \n",
      "1474   Easy of operation,storage of bits is very inno...   \n",
      "4966   This water bottle is so great!!!! I am able to...   \n",
      "6241   I liked the concept. It makes you free of tens...   \n",
      "474    From chew toys to bitter apple we tried everyt...   \n",
      "19606  Have purchased watches on-line before but been...   \n",
      "3316   Bought it, poured it in and it worked instantl...   \n",
      "2714   This is the coolest wallet I've ever owned in ...   \n",
      "8918   I made the mistake in purchasing another small...   \n",
      "5701   like to have handy on the floor board. it is a...   \n",
      "5948   If you live in an area that has a lot of winte...   \n",
      "4059   Its ok. I wouldnt buy it again. The net keeps ...   \n",
      "19103  Bought this for my wife for christmas 2012.  T...   \n",
      "...                                                  ...   \n",
      "6595   My son loves this... it works awesome & goes r...   \n",
      "4309   I needed to train fast for an upcoming charity...   \n",
      "3958   This should be described as a toy. I don't kno...   \n",
      "19404  These don't do anything had they been expensiv...   \n",
      "16400  This was a very cost-effective adapter set for...   \n",
      "1272   Good price looks ok except for the green parts...   \n",
      "3189   I have the complete collection of the original...   \n",
      "13293  My dog loves these.  They are the perfect size...   \n",
      "20497  I purchased this knowing that it was a little ...   \n",
      "8887   I bought this wallet for the design with the s...   \n",
      "1475   This bag's handles are non-existent. They are ...   \n",
      "2533   This is HORRIBLE!!! I ordered two of these gat...   \n",
      "13674  Installed this amplifier on an over the air an...   \n",
      "12004  Dimension: 51&#34;Wx18.5&#34;Dx15.5&#34;H<br /...   \n",
      "14480  I love this coffie and I should know coffie is...   \n",
      "3836   This cot seemed like the perfect solution for ...   \n",
      "13188  My refrigerator was not working although my fr...   \n",
      "3472   I love my bag. I carry it everywhere. It is ch...   \n",
      "5735   This book makes sense of that famous quote by ...   \n",
      "16532  Heinz Home Style is our favorite and has been ...   \n",
      "20188  I have bought many sony products and over time...   \n",
      "606    We put the battery in right when we got it and...   \n",
      "17601  I love how the legs of these fit.  The problem...   \n",
      "2826   Not good. faucets I bought have a leak and can...   \n",
      "13925  It is attractive, functional and fit perfectly...   \n",
      "1338   as a diabetic I have to be really careful what...   \n",
      "484    I bought them for my son as well. He's 5 month...   \n",
      "1719   I placed it in a very small bathroom and it lo...   \n",
      "19712  The picture quality is great on this tv, like ...   \n",
      "218    Assembly was pretty easy, although the provide...   \n",
      "\n",
      "                                              POS_COUNTS  NUM NOUNS  \\\n",
      "2312   {96: 3, 99: 7, 83: 5, 84: 6, 85: 5, 88: 2, 89:...          7   \n",
      "16845  {96: 7, 98: 1, 99: 11, 83: 9, 84: 9, 85: 4, 88...         21   \n",
      "6105   {96: 6, 99: 17, 83: 13, 84: 7, 85: 7, 88: 2, 8...         11   \n",
      "18082  {96: 6, 99: 9, 102: 5, 83: 1, 84: 4, 85: 1, 88...          7   \n",
      "11335  {96: 7, 99: 20, 83: 4, 84: 10, 85: 3, 88: 3, 8...         13   \n",
      "17677  {96: 5, 99: 8, 102: 2, 83: 6, 84: 4, 85: 5, 88...          8   \n",
      "1092   {96: 4, 99: 2, 83: 3, 84: 2, 85: 1, 88: 1, 89:...          9   \n",
      "8364   {96: 6, 99: 21, 83: 8, 84: 9, 85: 7, 88: 1, 89...         13   \n",
      "18550  {96: 6, 98: 1, 99: 13, 102: 3, 83: 8, 84: 3, 8...          7   \n",
      "11423  {96: 4, 99: 14, 102: 3, 83: 5, 84: 5, 85: 3, 8...         15   \n",
      "6757   {96: 5, 99: 4, 83: 7, 84: 6, 85: 5, 88: 1, 89:...          6   \n",
      "12893  {96: 5, 99: 5, 83: 3, 84: 1, 85: 3, 88: 1, 89:...          1   \n",
      "17446  {96: 8, 99: 21, 83: 8, 84: 14, 85: 7, 89: 12, ...         15   \n",
      "11823  {96: 7, 99: 9, 83: 6, 84: 1, 85: 4, 88: 6, 89:...          8   \n",
      "4772   {96: 4, 99: 5, 83: 5, 84: 6, 85: 2, 88: 1, 89:...          9   \n",
      "18882  {96: 3, 99: 7, 102: 1, 83: 3, 85: 3, 88: 2, 89...          5   \n",
      "7700   {96: 8, 99: 28, 83: 9, 84: 8, 85: 10, 88: 1, 8...         10   \n",
      "5018   {96: 8, 99: 19, 83: 14, 84: 9, 85: 10, 88: 5, ...         15   \n",
      "1474   {96: 4, 98: 1, 99: 4, 83: 3, 84: 3, 85: 3, 89:...          7   \n",
      "4966   {96: 9, 99: 17, 83: 9, 84: 9, 85: 5, 88: 2, 89...         16   \n",
      "6241   {96: 2, 99: 3, 83: 2, 84: 4, 85: 3, 88: 1, 89:...          5   \n",
      "474    {96: 5, 99: 15, 83: 7, 84: 6, 85: 5, 88: 2, 89...         13   \n",
      "19606  {96: 9, 99: 10, 83: 8, 84: 3, 85: 5, 88: 2, 89...         10   \n",
      "3316   {96: 6, 99: 12, 83: 1, 84: 3, 85: 6, 88: 1, 89...          5   \n",
      "2714   {96: 13, 99: 28, 102: 1, 83: 13, 84: 15, 85: 1...         14   \n",
      "8918   {96: 4, 99: 8, 83: 8, 84: 5, 85: 3, 88: 2, 89:...         11   \n",
      "5701   {96: 4, 99: 7, 83: 4, 84: 2, 85: 2, 89: 1, 90:...          5   \n",
      "5948   {96: 16, 99: 25, 102: 1, 83: 11, 84: 19, 85: 9...         35   \n",
      "4059   {96: 4, 99: 8, 83: 5, 84: 2, 85: 3, 88: 1, 89:...          6   \n",
      "19103  {96: 6, 99: 9, 102: 4, 83: 3, 84: 4, 85: 5, 88...          9   \n",
      "...                                                  ...        ...   \n",
      "6595   {96: 7, 99: 7, 83: 5, 84: 1, 85: 3, 88: 2, 89:...          4   \n",
      "4309   {96: 7, 99: 13, 83: 5, 84: 10, 85: 8, 88: 4, 8...         12   \n",
      "3958   {96: 10, 99: 22, 83: 4, 84: 9, 85: 14, 88: 3, ...          9   \n",
      "19404  {96: 1, 99: 8, 83: 2, 84: 2, 85: 3, 88: 1, 89:...          3   \n",
      "16400  {96: 9, 98: 2, 99: 22, 83: 8, 84: 14, 85: 5, 8...         16   \n",
      "1272   {96: 6, 99: 6, 83: 9, 84: 6, 85: 4, 88: 2, 89:...          8   \n",
      "3189   {96: 7, 99: 10, 83: 9, 84: 6, 85: 6, 88: 2, 89...         12   \n",
      "13293  {96: 9, 99: 17, 102: 6, 83: 7, 84: 10, 85: 6, ...         10   \n",
      "20497  {96: 29, 98: 3, 99: 46, 83: 15, 84: 17, 85: 12...         37   \n",
      "8887   {96: 4, 98: 1, 99: 6, 83: 1, 84: 2, 85: 2, 88:...          5   \n",
      "1475   {96: 4, 99: 10, 83: 4, 84: 4, 85: 4, 88: 1, 89...          8   \n",
      "2533   {96: 8, 99: 12, 83: 5, 84: 1, 85: 8, 88: 4, 89...          5   \n",
      "13674  {96: 7, 99: 12, 102: 5, 83: 5, 84: 8, 85: 5, 8...         27   \n",
      "12004  {96: 24, 98: 1, 99: 85, 100: 2, 83: 34, 84: 38...         52   \n",
      "14480  {96: 1, 99: 11, 83: 2, 84: 1, 85: 4, 88: 2, 89...          3   \n",
      "3836   {96: 2, 99: 9, 83: 4, 84: 5, 85: 2, 88: 2, 89:...          6   \n",
      "13188  {96: 8, 99: 18, 102: 4, 83: 8, 84: 7, 85: 4, 8...         13   \n",
      "3472   {96: 5, 99: 6, 100: 1, 83: 4, 84: 3, 85: 1, 89...          6   \n",
      "5735   {96: 5, 99: 3, 83: 2, 84: 7, 88: 1, 89: 3, 91:...          8   \n",
      "16532  {96: 1, 99: 4, 83: 3, 84: 2, 85: 1, 88: 1, 89:...          2   \n",
      "20188  {96: 9, 99: 14, 83: 3, 84: 10, 85: 5, 88: 3, 8...         17   \n",
      "606    {96: 3, 99: 10, 84: 2, 85: 5, 88: 2, 89: 2, 91...          3   \n",
      "17601  {96: 3, 99: 6, 102: 2, 83: 3, 84: 3, 85: 6, 88...          4   \n",
      "2826   {96: 4, 99: 7, 83: 2, 84: 1, 85: 4, 88: 1, 89:...          4   \n",
      "13925  {96: 7, 99: 7, 83: 7, 84: 5, 85: 4, 88: 1, 89:...          7   \n",
      "1338   {96: 5, 99: 25, 100: 1, 83: 13, 84: 12, 85: 8,...         15   \n",
      "484    {96: 6, 99: 8, 83: 6, 84: 3, 85: 3, 88: 1, 89:...          6   \n",
      "1719   {96: 2, 99: 4, 83: 4, 84: 1, 85: 2, 88: 1, 89:...          2   \n",
      "19712  {96: 2, 99: 5, 83: 7, 84: 6, 85: 2, 88: 3, 89:...         12   \n",
      "218    {96: 8, 99: 12, 83: 7, 84: 4, 85: 7, 88: 1, 89...          7   \n",
      "\n",
      "       NUM VERBS  NUM ADJECTIVES  NUM ADVERBS  REVIEW_LENGTH  SENTIMENT SCORE  \n",
      "2312           7               5            5            199           0.9570  \n",
      "16845         11               9            4            414           0.9393  \n",
      "6105          17              13            7            371           0.8581  \n",
      "18082          9               1            1            195          -0.7096  \n",
      "11335         20               4            3            353           0.6124  \n",
      "17677          8               6            5            240           0.8709  \n",
      "1092           2               3            1            122           0.7264  \n",
      "8364          21               8            7            416           0.8532  \n",
      "18550         13               8            6            312          -0.7351  \n",
      "11423         14               5            3            319           0.8658  \n",
      "6757           4               7            5            197          -0.0790  \n",
      "12893          5               3            3            109           0.9538  \n",
      "17446         21               8            7            462           0.8555  \n",
      "11823          9               6            4            214           0.9429  \n",
      "4772           5               5            2            201          -0.5859  \n",
      "18882          7               3            3            139           0.0000  \n",
      "7700          28               9           10            455           0.9538  \n",
      "5018          19              14           10            462           0.9615  \n",
      "1474           4               3            3            141           0.7264  \n",
      "4966          17               9            5            414           0.9301  \n",
      "6241           3               2            3            120           0.7430  \n",
      "474           15               7            5            358          -0.2263  \n",
      "19606         10               8            5            249           0.9599  \n",
      "3316          12               1            6            195          -0.4019  \n",
      "2714          28              13           11            561           0.8837  \n",
      "8918           8               8            3            297           0.5269  \n",
      "5701           7               4            2            146           0.8591  \n",
      "5948          25              11            9            702           0.9760  \n",
      "4059           8               5            3            166           0.6249  \n",
      "19103          9               3            5            246          -0.4659  \n",
      "...          ...             ...          ...            ...              ...  \n",
      "6595           7               5            3            150           0.9294  \n",
      "4309          13               5            8            383           0.8979  \n",
      "3958          22               4           14            386           0.4255  \n",
      "19404          8               2            3            107          -0.3753  \n",
      "16400         22               8            5            490          -0.5795  \n",
      "1272           6               9            4            235           0.7636  \n",
      "3189          10               9            6            334           0.9195  \n",
      "13293         17               7            6            402           0.9285  \n",
      "20497         46              15           12           1225           0.9586  \n",
      "8887           6               1            2            156           0.0000  \n",
      "1475          10               4            4            202          -0.2878  \n",
      "2533          12               5            8            243          -0.9402  \n",
      "13674         12               5            5            451           0.8538  \n",
      "12004         85              34           37           1763           0.9900  \n",
      "14480         11               2            4            147           0.8870  \n",
      "3836           9               4            2            190           0.5789  \n",
      "13188         18               8            4            348           0.8553  \n",
      "3472           6               4            1            157           0.9698  \n",
      "5735           3               2            0            242           0.0000  \n",
      "16532          4               3            1            100           0.1280  \n",
      "20188         14               3            5            389           0.0000  \n",
      "606           10               0            5            138           0.0000  \n",
      "17601          6               3            6            159           0.6705  \n",
      "2826           7               2            4            107          -0.6675  \n",
      "13925          7               7            4            222           0.9092  \n",
      "1338          25              13            8            495           0.9524  \n",
      "484            8               6            3            175           0.8481  \n",
      "1719           4               4            2            101           0.3119  \n",
      "19712          5               7            2            237           0.7187  \n",
      "218           12               7            7            277           0.9274  \n",
      "\n",
      "[1000 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d67e4dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312         Fake\n",
      "16845    Not Fake\n",
      "6105         Fake\n",
      "18082    Not Fake\n",
      "11335    Not Fake\n",
      "17677    Not Fake\n",
      "1092         Fake\n",
      "8364         Fake\n",
      "18550    Not Fake\n",
      "11423    Not Fake\n",
      "6757         Fake\n",
      "12893    Not Fake\n",
      "17446    Not Fake\n",
      "11823    Not Fake\n",
      "4772         Fake\n",
      "18882    Not Fake\n",
      "7700         Fake\n",
      "5018         Fake\n",
      "1474         Fake\n",
      "4966         Fake\n",
      "6241         Fake\n",
      "474          Fake\n",
      "19606    Not Fake\n",
      "3316         Fake\n",
      "2714         Fake\n",
      "8918         Fake\n",
      "5701         Fake\n",
      "5948         Fake\n",
      "4059         Fake\n",
      "19103    Not Fake\n",
      "           ...   \n",
      "6595         Fake\n",
      "4309         Fake\n",
      "3958         Fake\n",
      "19404    Not Fake\n",
      "16400    Not Fake\n",
      "1272         Fake\n",
      "3189         Fake\n",
      "13293    Not Fake\n",
      "20497    Not Fake\n",
      "8887         Fake\n",
      "1475         Fake\n",
      "2533         Fake\n",
      "13674    Not Fake\n",
      "12004    Not Fake\n",
      "14480    Not Fake\n",
      "3836         Fake\n",
      "13188    Not Fake\n",
      "3472         Fake\n",
      "5735         Fake\n",
      "16532    Not Fake\n",
      "20188    Not Fake\n",
      "606          Fake\n",
      "17601    Not Fake\n",
      "2826         Fake\n",
      "13925    Not Fake\n",
      "1338         Fake\n",
      "484          Fake\n",
      "1719         Fake\n",
      "19712    Not Fake\n",
      "218          Fake\n",
      "Name: LABEL, Length: 1000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# # Transform the testing data\n",
    "X_TEST_TRANSFORMED = feature_union.transform(X_TEST)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4b38313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_TRAIN_TRANSFORMED, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5d48243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.87      0.69      0.77        94\n",
      "    Not Fake       0.77      0.91      0.83       106\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       200\n",
      "   macro avg       0.82      0.80      0.80       200\n",
      "weighted avg       0.81      0.81      0.80       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/linear_model/base.py:283: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_TEST_TRANSFORMED)\n",
    "print(classification_report(Y_TEST, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b50e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed989188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33921fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MAX_SIMILARITY'] = df1['REVIEW_TEXT'].apply(lambda x: max(df1.loc[df1['REVIEW_TEXT'] != x, 'REVIEW_TEXT'].apply(lambda y: calculate_cosine_similarity(x, y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cfdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
